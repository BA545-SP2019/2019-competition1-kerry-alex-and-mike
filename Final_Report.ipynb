{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report for Competition 1: IPO Price Determination #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question ##\n",
    "What are the determinants of the IPO underpricing phenomena?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Imputation & Creation of Ratio Variables**\n",
    "\n",
    "We first examined the dataframe to get a better understanding of the data using .info, .describe() and .isnull(). These methods told us the datatype of each column, descriptive statistics and if there were any missing values. In this dataset the missing values were represented with the value ‘-‘ and not as NaN. We replaced the ‘-‘ values with NaN to get more accurate counts of the missing values. \n",
    "Next, we imputed the missing values using the mean and the mode. The only feature that was imputed with the mode is C2 because it is a binary variable all of the other features were imputed using the mean because they are continuous variables. We ran descriptive statistics on the imputed data to compare to the previous descriptive statistics to see if any major changed happened. \n",
    "The last section in this step was the creation of new variables C3p, C5p, C6p, T3p, T4p, T5p, S1p, S2p and S3p. These new ratio variables then took the place of the original variables on the dataframe. We did a final check for the creation of new variables, missing values and dtypes before exporting the dataframe to a csv file to be used in the next step. Imputation notebook can be found here: [Impuation Notebook](pipeline3/p3imputation.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Outlier Handling using IQR**\n",
    "\n",
    "In this pipeline we handle outlier right after imputation. We went through the process of finding the Q1, Q3 and IQR for each column which are needed to find the upper and lower bounds. We define outliers as any value that falls above the upper bound or below the lower bound of each column. We used a for loop and a function to replace any outlier values with the upper or lower bound depending on the outlier value. To verify that outlier values were properly replaced we examined the boxplots for each variable, and we can see that there are no values that fall outside the minimum or maximum values as depicted by the whiskers of the boxplot. The new dataframe with no outlier values was then exported to a csv to be used in the next step. IQR notebook can be found here: [IQR Notebook](pipeline3/p3IQR.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Normalization using Min/Max Scaler**\n",
    "\n",
    "In order to normalize the dataset to be able to compare across columns and start to create an\n",
    "ideal model, we coded a method to take a column of data, find the mean of the column, the\n",
    "min, the max, and calculate the z-score from these measures. After creating this method, we\n",
    "create a Data frame with the dataset in order to use a for loop and go through each column in\n",
    "the Data frame and apply the normalize method on each. After importing preprocessing from sklearn,  we were able to use the MinMaxScaler() package to go finish normalizing the data by scaling the column values between 0 and 1. Normalization notebook can be found here: [Normalization Notebook](pipeline3/p3minmax.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Fixing the Skew**\n",
    "\n",
    "Before our data could be used in any feature selection processes, we had to fix the skew of the data to fall between the range of -0.5 and 0.5 or as close to the range as possible so each had a more normal distribution. We first examined the histograms to see the skew of each variable and we also printed out the skew value of each variables using the .skew() method. The only variables we needed to adjust were C6p, C7 and S2p because their skew values were too far outside the acceptable range. After fixing the skew, we re-examined the histograms for each variable and can see that the variables are now normally distributed or as close to normally distributed as possible. We exported the new dataframe to a csv file to be used later. Skew notebook can be found here: [Skew Notebook](pipeline3/p3skewness.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Binning I3**\n",
    "\n",
    "To make the ‘I3’ column easier to interpret and compare to others in our evaluations we decided\n",
    "to bin it. I3 was a Standardized Industrial Classifier (STI) that consisted of four numbers. First, we had to handle eight missing values in the column. We identified which row the value was missing from and the name of the company. We googled the company names in order to find the STI for the missing companies. After doing that we used an .iloc function in order fill the I3 cell in each of the eight different rows that were missing. We decided to bin the column by industry which was represented by the first two numbers of column. We first did a value counts on the I3 column to see which values were represented the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating the results, we decided that it would be best to bin our results into 3 bins. In order to bin we created a function that would evaluate the value in I3 and if it was between 2,000 and 3,999 it would return 1, if it was between 7,000 and 8,999 it would return 2, and another other number it would return 3. One represents the company being in the manufacturing industry, two represents the company being in the service industry, and three represents all other industries. Once we created the function, we then created a new column in the dataFrame named ‘Industry_Bins’ and applied the function that we made to column ‘I3’ in order to return a new value. Once this was done we had all of the companies in the dataset successfully binned. A link to the binning notebook can be found here: [Binning I3 Notebook](pipeline3/p3binning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Correlation**\n",
    "\n",
    "Once we cleaned our dataset and added in all the necessary new columns we ran a correlation in order to see how each variable related to one another. The correlation method that we used was ‘Spearman’. We were looking to see if there were any variables that were highly correlated to either of our target variables, those being Y1 and Y2. We defined a variable are being highly\n",
    "correlated if the result was either higher than 0.5 or lower than -0.5. If a variable fell under either of these in relations to either of our target variables then they would be seen as highly correlated. Originally we made a large table of the correlation of every variable to each other; however, the large amount of columns made this result hard to interpret. To make it easier to observe we just looked at each of the two target variables individually and evaluated if the correlation of any of the other variables to them was higher than 0.5, which would return a ‘True’ result. Through these two tests we found that none of our variables were highly correlated to either Y1 or Y2. We also made a heat map using the seaborn package that enabled us to visualize how strong or weakly correlated each variable was to each other. While we did not have any variables that were\n",
    "highly correlated to our two target variables this is not a bad thing it just simply showed us that one variable alone would not be a good predictor of our target variables. Our correlation notebook can be found here: [Correlation Notebook](pipeline3/p3correlation.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing Steps ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning all continuous variables. We used the KBinsDiscretizer and the OneHotEncoder to make all continuous variables into binary variables. Once we did this we ran the same RFE notebook to determine the best performing variables and ran those combinations through the evaluation code. We found that binning did not have any effect on the F1 and AUC scores and decided to not continue with this step. The continuous binning notebook can be found here: [Continuous Binning Notebook](Binning/p3continuous-binning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1: Filtering**\n",
    "\n",
    "One of the feature selection methods that we employed was filtering. The filtering method that we decided to use was Chi-Square(Chi2) since after class we thought that it would be a good method to use. We first started by splitting our DataFrame into three different arrays. This would make it easier when we ran the data through sklearn. Our first array, x, was made up of every\n",
    "variable except our two target variables. Our second array, y1, contained out first target variable, Y1. Our last array, y2, had our second target variable, Y2. We used the SelectKBest feature from sklearn in order to calculate and return the chi2 score for each variable for us. We used the same code that you provided to us in the feature selection notebook in order to help us calculate the chi2 for both our Y1 and Y2 in relation to X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We originally set the number of variables to be returned to four, but we used the summary results for both Y1 and Y2 to help us evaluate which variables to select for our feature selection. The feature variables selected to be included from running chi2 for Y1 and Y2 are the columns that received the highest scores. If we wanted the top three features to include we would choose the three columns that returned the highest score from X in comparison to Y1 and Y2 individually and if we wanted the top seven variables it would be the seven columns that returned the highest scores. We ended up choosing between three and seven variables from the results of running chi2. We then ran the five different versions of variables through our evaluation code. The filtering notebooks can be found here: [Filtering](pipeline3/p3Filtering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2: Wrapping-Recursive Feature Elimination (RFE)**\n",
    "\n",
    "Recursive Feature Elimination (RFE) is a wrapping feature selection method that is used to find the variables that produce the best performance. This method will go through every possible combination of features and will identify the best and worst performing features in each combination. After every feature combination is explored the RFE method will rank the features on their order or elimination. We used the RFE code found in Dr. Tao’s “Feature Selection Notebooks” from week four in class. We ran the results from the combinations with 1 to 9 variables through the evaluation code to get F1 and AUC scores. From these results we chose the combination that produced the highest F1 and AUC scores. RFE notebook can be found here: \n",
    "- [RFE Y1](pipeline3/RFE-y1-p3.ipynb)\n",
    "- [RFE Y2](pipeline3/RFE-y2-p3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 3: Embedding-Ridge Regression**\n",
    "\n",
    "Ridge Regression is the last type of feature selection that is often used to prevent or reduce overfitting and collinearity between variables. In order to do this, Ridge Regression focuses on reducing the size that a coefficient can be, therefore reducing the introduction of more error, collinearity, and overall complexity of the model being fitted. We used the code from Professor Tao’s example notebooks for feature selection to outline the code for Ridge Regression. This starts off with importing numpy and pandas, in addition to Ridge from the sklearn.linear_model package. After importing the csv for the specific pipeline and assigning it to a data frame, we create a numpy array with this data frame and define the target variable and the other data values to be tested. The code runs the Ridge package with an alpha of 1.0 and assign it to an object which will then be used to fit the data in the data frame we defined as an array. A method is defined to print out the Ridge Regression calculations for each coefficient in a useful way to read each coefficient and determine whether to keep it in the model. We run this method with on the fitted Ridge data frame and print out the coefficient terms that are evaluated from the data provided. Any coefficients that are very low and close to zero, negative, or zero can be assumed to not contribute to predicting the target variable and therefore should be removed from the final model. The Ridge Regression notebook can be found here: [Ridge Regression](pipeline3/p3Embedded-RidgeRegression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**\n",
    "\n",
    "Decision trees are another way to carry out feature selection. They are very thorough in evaluating every possible combination of values and variables to find the best possible accuracy of predicting the target variable. It allows you to try out many different paths in large datasets to find the best combination of variables and methods by evaluating every possible result. In addition to showing the best outcome, decision trees are able to give an accuracy measure for each result to show how likely those outcomes are to occur. Decision trees are often used for feature selection because once the target variables are defined and the rest of the data is fitted to a testing dataset, the evaluation of accuracy comes fairly easy with the Python Decision tree packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code, we started off by importing all of the needed imports, including the DecisionTreeClassifier from sklearn, to carry out the actual decision tree evaluation, the train_test_split from sklearn.model_selection to properly split our dataset into testing and training datasets, and metrics from sklearn to calculate the accuracy measure. Next, we read in the csv specifically for each pipeline and assigned it to a DataFrame, followed by assigning one target variable column to a variable and the rest of the columns to another variable. Training and testing data frames were created using the train_test_split package and the variables we just assigned. We also test sizes and random_states to find the best accuracy. Lastly, we ran the DecisionTreeClassifier and assigned it to an object, fit that object with the training data frames, and used the predict method in the DecisionTreeClassifier object to assign a prediction based on the training data fit. A best accuracy score was printed out to show the best possible accuracy score that could predict the target variable with the dataset it was given at the very\n",
    "beginning and all of the criteria applied to it. We then duplicated these actions for the second target variable. Our final scores for the Decision Tree were **0.7080 for Y1 and 0.7592 for Y2**. The Decision Tree notebook can be found here: [Decision Tree](pipeline3/DecisionTreeP3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the preprocessing and feature selection methods, we have determined that the predictors of **Y1** are **S2p, T3p, C4, T4p, T5p, C5p and C2 (indexes 0 to 6)** and the predictors of **Y2** are **C6p, T5p, S2p, C4, S3p, T3p, S1p and the Industry Bins (indexes 3 to 10**. The final csv file is named **final.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C2 = Top tier Dummmy \n",
    "- C4 = Prior Nasdaq 15-Day Returns \n",
    "- C5p = Share Overhang\n",
    "- C6p = Up Revision Percentage\n",
    "- T3p = Percent of Real Words\n",
    "- T4p = Percent of Long Sentences \n",
    "- T5p = Percent of Long Words\n",
    "- S1p = Percent of Positive Words\n",
    "- S2p = Percent of Negative Words \n",
    "- S3p = Percent of Uncertain Words\n",
    "- Industry Bins = binned I3 value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
